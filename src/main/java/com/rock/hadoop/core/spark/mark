【】Spark与MapReduce区别
MapReduce：基于磁盘进行计算，中间计算的结果都是存储在磁盘中，每次计算都是从磁盘中来加载数据。
    （一次 MapReduce 运算之后,会将数据的运算结果从内存写入到磁盘中,第二次 Mapredue 运算时在从磁盘中读取数据,所以其瓶颈在2次运算间的多余 IO 消耗）
    Hadoop 的 Map 和 reduce 之间的 shuffle 需要 sort。

Spark：Spark 则是将数据一直缓存在内存中,直到计算得到最后的结果,再将结果写入到磁盘,所以多次运算的情况下, Spark 是比较快的.
    当内存不足时，可以溢出到磁盘上。

【】spark介绍
Spark 是使用 scala 实现的基于内存计算的大数据开源集群计算环境.

Spark 集群中,分为 Master 节点与 worker 节点,,其中 Master 节点上常驻 Master 守护进程和 Driver 进程,
Master 负责将串行任务变成可并行执行的任务集Tasks, 同时还负责出错问题处理等,而 Worker 节点上常驻 Worker 守护进程,
Master 节点与 Worker 节点分工不同, Master 负载管理全部的 Worker 节点,而 Worker 节点负责执行任务.

Spark 支持不同的运行模式,包括Local, Standalone,Mesoses,Yarn 模式.不同的模式可能会将 Driver 调度到不同的节点上执行.

ps:
运行程序前得安装spark环境，由于是使用Scala编写的，仅是提供了java API的接口，具体的执行还是得Scala程序来运行，因此需要先安装。---验证不安装也可以本地运行


【】运行方式
方式1：spark-submit --class com.xxxx.App xxxxx.jar --master local[3]  --验证OK

【】SparkSQL
SparkSQL可以理解为在原生的RDD上做的一层封装，通过SparkSQL可以在scala和java中写SQL语句，Spark SQL也可以用来从Hive中读取数据，
并将结果作为Dataset/DataFrame返回。简单来讲，SparkSQL可以让我们像写SQL一样去处理内存中的数据。
它是用来处理结构化的数据，例如具有schema结构的数据，json, parquet, avro, csv格式的。
比如txt的等非格式化数据可以通过spark转成指定格式化的数据，再通过sql进行查询。

通过spark sql ，可以使用SQL 或者 HQL 来查询数据，查询结果以Dataset/DataFrame 的形式返回
它支持多种数据源，如Hive 表、Parquet 以及 JSON 等
它支持开发者将SQL 和传统的RDD 变成相结合

产生背景
我们可以直接通过写sql来分析大数据中的数据。
hive是把sql翻译成mapreduce作业，mapreduce执行效率是不高的（当数据量比较大，执行时间可能十几个小时）


【问题】
1、mp和spark在各个环境如何来运行--本地、机群等方式
2、好的场景使用spark来构建项目？
3、spark等计算框架原理思考贯通？

